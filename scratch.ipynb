{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scratch space for TKG patterns synthetic data creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "\n",
    "from collections import defaultdict\n",
    "from itertools import combinations, product\n",
    "\n",
    "import os\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "1. Start with random graph. Parameters: # of entities, # of relations, # of time windows, avg density of entities \n",
    "2. Choose set of patterns. Start w/ 3-hop, 2-hop, 1-hop. Pick time-stamp diffs. Don't allow subset patterns (e.g. chosen 2-hop patterns can't be included in 3-hop)\n",
    "3. Apply patterns iteratively. Parameters: Probability of random wiring, probability of no time pattern application (or interrupted time pattern application)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalPattern():\n",
    "    def __init__(\n",
    "        self,\n",
    "        antecedent: 'List[Tuple[int,int,int]]' = [],\n",
    "        consequence: 'Tuple[int,int,int]' = None,\n",
    "        time_lags: 'List[Tuple[float,float]]' = [],\n",
    "        n_hops: int = None,\n",
    "    ):\n",
    "        \"\"\" Defines a temporal pattern over our TKG\n",
    "        Args:\n",
    "            antecedent (List[Tuple[int,int,int]]): Antecedent(s) for the pattern\n",
    "                in the form of subject, relation, object ID triples\n",
    "            consequence (Tuple[int,int,int]): Consequence for the pattern in the form\n",
    "                of a subject, relation, object ID triple\n",
    "            time_lags (List[Tuple[float,float]]): Time lags with which antecedents and \n",
    "                consequences can occur validly in the pattern. Must be an iterable of\n",
    "                length equal to the antecedent. The i-th element of time_lags is a\n",
    "                tuple of the form (minimum # of time windows since i-th antecedent,\n",
    "                maximum # of time windows since i-th antecedent) for the i+1-th\n",
    "                antecedent (or consequence if we have iterated over all antecedents)\n",
    "        \"\"\"\n",
    "        self.antecedent = antecedent\n",
    "        self.consequence = consequence\n",
    "        self.time_lags = time_lags\n",
    "        self.n_hops = n_hops\n",
    "\n",
    "        # Regex pattern used to parametrize a pattern from a text label\n",
    "        self.pat_time_lag = re.compile('t\\d+=t\\d+\\+\\((\\d+),\\s(\\d+)\\)')\n",
    "    \n",
    "    def __label__(self) -> str:\n",
    "        \"\"\" Return a string label for the pattern like <ANTECEDENT> -> <CONSEQUENCE>\n",
    "        \"\"\"\n",
    "        label = ''\n",
    "        for idx in range(len(self.antecedent)):\n",
    "            if idx > 0:\n",
    "                label += ' & '\n",
    "            label += f'({self.antecedent[idx][0]}, '\n",
    "            label += f'{self.antecedent[idx][1]}, '\n",
    "            label += f'{self.antecedent[idx][2]}, '\n",
    "            label += f't{idx+1}{f\"=t{idx}+\"+str(self.time_lags[idx-1]) if idx > 0 else \"\"})'\n",
    "        label += ' -> '\n",
    "        label += f'({self.consequence[0]}, '\n",
    "        label += f'{self.consequence[1]}, '\n",
    "        label += f'{self.consequence[2]}, '\n",
    "        label += f't{len(self.antecedent)+1}{f\"=t{len(self.antecedent)}+\"+str(self.time_lags[-1])})'\n",
    "        return label\n",
    "    \n",
    "    def __triples__(self) -> 'List[Tuple[int,int,int]]':\n",
    "        \"\"\" Return antecedent and consequence as list of triples (excluding time lag information)\n",
    "        \"\"\"\n",
    "        return self.antecedent + [self.consequence]\n",
    "    \n",
    "    def __quadruples__(self) -> 'List[Tuple[int,int,int,Tuple[float,float]]]':\n",
    "        \"\"\" Return antecedent and consequence as list of quadruples (including time lag information)\n",
    "        \"\"\"\n",
    "        return [\n",
    "            triple + (time_lag,) for triple, time_lag in zip(self.__triples__(), [()]+self.time_lags)\n",
    "        ]\n",
    "    \n",
    "    def from_label(self, label: str):\n",
    "        \"\"\" Parametrize a pattern from a string label (as generated by __label__ method)\n",
    "        \"\"\"\n",
    "        antecedent, consequence = label.split('->')\n",
    "        antecedent = antecedent.split('&')\n",
    "        # Get time lag information\n",
    "        time_lags = [\n",
    "            (\n",
    "                int(self.pat_time_lag.search(text).groups()[0]),\n",
    "                int(self.pat_time_lag.search(text).groups()[1])\n",
    "            )\n",
    "            for text in antecedent+[consequence]\n",
    "            if self.pat_time_lag.search(text)\n",
    "        ]\n",
    "        # Format antecedent and consequence\n",
    "        antecedent = [\n",
    "            eval('('+','.join(ant.strip(' ()').split(',')[:3])+')')\n",
    "            for ant in antecedent\n",
    "        ]\n",
    "        consequence = eval('('+','.join(consequence.strip(' ()').split(',')[:3])+')')\n",
    "        \n",
    "        self.antecedent = antecedent\n",
    "        self.consequence = consequence\n",
    "        self.time_lags = time_lags\n",
    "        self.n_hops = len(antecedent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_subpattern(subpattern: 'List[Tuple]', patterns: 'List[Tuple]') -> bool:\n",
    "    \"\"\" Test whether subpattern is a subpattern of any member of patterns\n",
    "    \"\"\"\n",
    "    n = len(subpattern)\n",
    "    pattern_subsets = set([\n",
    "        tuple(pattern[idx:idx+n]) for pattern in patterns\n",
    "        for idx in range(len(pattern)-n+1)\n",
    "    ])\n",
    "    if tuple(subpattern) in pattern_subsets:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entities_intersect(entities1: 'List[int]', entities2: 'List[int]') -> bool:\n",
    "    \"\"\" Indicate whether entities1 and entities2 have any intersection\n",
    "    \"\"\"\n",
    "    if len(set(entities1).intersection(entities2)) > 0:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entities_connect_triples(\n",
    "    e1: int, e2: int, triple1: 'Tuple[int,int,int]', triple2: 'Tuple[int,int,int]'\n",
    ") -> bool:\n",
    "    \"\"\" Indicate whether entities e1 and e2 connect triples triple1 and triple2\n",
    "    \"\"\"\n",
    "    if (e1 in {triple1[0], triple1[2]}) & (e2 in {triple2[0], triple2[2]}):\n",
    "        return True\n",
    "    elif (e2 in {triple1[0], triple1[2]}) & (e1 in {triple2[0], triple2[2]}):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entities_connect_components(\n",
    "    e1: int, e2: int, comp1: 'List[Tuple[int,int,int]]', comp2: 'List[Tuple[int,int,int]]'\n",
    ") -> bool:\n",
    "    \"\"\" Indicate whether entities e1 and e2 connect components comp1 and comp2\n",
    "    \"\"\"\n",
    "    for triple1, triple2 in product(comp1, comp2):\n",
    "        if entities_connect_triples(e1, e2, triple1, triple2):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combinations_of_increasing_size(iterable, a, b):\n",
    "    \"\"\" Return combinations of iterable of size from a to b, inclusive\n",
    "    \"\"\"\n",
    "    all_combinations = []\n",
    "    for size in range(a, b+1):\n",
    "        combs = combinations(iterable, size)\n",
    "        for comb in combs:\n",
    "            yield comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def force_swap_to_entities(\n",
    "    idxs_to_force: 'List[int]',\n",
    "    sampled_entities: 'List[int]',\n",
    "    swap_to_entities: 'List[int]',\n",
    "    seed: int = None,\n",
    ") -> None:\n",
    "    \"\"\" Force at least one of idxs_to_swap to be switching in sampled_entities\n",
    "    to one of swap_to_entities. Note, alters swap_to_entities in place.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    to_swap = random.choice(\n",
    "        list(combinations_of_increasing_size(idxs_to_force, 1, len(idxs_to_force)))\n",
    "    )\n",
    "    for idx in to_swap:\n",
    "        swap_to = random.choice(swap_to_entities)\n",
    "        sampled_entities[idx] = swap_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def force_connect_components(\n",
    "    idxs_to_force: 'List[int,int]',\n",
    "    sampled_entities: 'List[int]',\n",
    "    comp1: 'List[int]',\n",
    "    comp2: 'List[int]',\n",
    "    seed: int = None,\n",
    ") -> None:\n",
    "    \"\"\" Force at least one of idxs_to_force to be switched so as to connect comp1\n",
    "    and comp2. Note, alters sampled_entities in place.\n",
    "    \"\"\"\n",
    "    if len(idxs_to_force) != 2:\n",
    "        raise ValueError(\n",
    "            'force_connecte_components only implemented for idxs_to_force of length 2'\n",
    "        )\n",
    "    random.seed(seed)\n",
    "    comps = [comp1, comp2]\n",
    "    if sampled_entities[idxs_to_force[0]] in comps[0]:\n",
    "        sampled_entities[idxs_to_force[1]] = random.choice(comps[1])\n",
    "    elif sampled_entities[idxs_to_force[0]] in comps[1]:\n",
    "        sampled_entities[idxs_to_force[1]] = random.choice(comps[0])\n",
    "    elif sampled_entities[idxs_to_force[1]] in comps[0]:\n",
    "        sampled_entities[idxs_to_force[0]] = random.choice(comps[1])\n",
    "    elif sampled_entities[idxs_to_force[1]] in comps[1]:\n",
    "        sampled_entities[idxs_to_force[0]] = random.choice(comps[0])\n",
    "    else:\n",
    "        random.shuffle(comps)\n",
    "        sampled_entities[idxs_to_force[0]] = random.choice(comps[0])\n",
    "        sampled_entities[idxs_to_force[1]] = random.choice(comps[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_lag_tuples(\n",
    "    time_lags: 'List[Tuple(float,float)]',\n",
    "    antecedent: 'List[Tuple[int,int,int]]',\n",
    ") -> 'List[Tuple[float,float]]':\n",
    "    \"\"\" Create time_lag_tuples used to instantiate patterns. Contains logic to prohibit: identical\n",
    "    antecedents from having 0 time lag between them, the consequence from having 0 lag from the\n",
    "    last antecedent.\n",
    "    \"\"\"\n",
    "    time_lag_tuples = []\n",
    "    for idx, time_lag in enumerate(time_lags):\n",
    "        lag_min, lag_max = time_lag[0], time_lag[1]\n",
    "        time_lag_tuple = []\n",
    "        if type(lag_min) in [float, int]:\n",
    "            pass\n",
    "        else:\n",
    "            lag_min = lag_min()\n",
    "        if type(lag_max) in [float, int]:\n",
    "            pass\n",
    "        else:\n",
    "            lag_max = lag_max()\n",
    "        # Prohibit identical antecedents from having 0 lag_min between them\n",
    "        if (idx < len(antecedent)-1) and (antecedent[idx] == antecedent[idx+1]):\n",
    "            lag_min = max(1, lag_min)\n",
    "        # Prohibit consequence from having 0 lag_min from the last antecedent\n",
    "        if idx == len(antecedent)-1:\n",
    "            lag_min = max(1, lag_min)\n",
    "        # Enforce that lag_max is no smaller than lag_min\n",
    "        time_lag_tuples.append((lag_min, max(lag_min, lag_max)))\n",
    "    return time_lag_tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define basic information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ents = 10\n",
    "n_rels = 5\n",
    "n_tws = 10\n",
    "entity2id = pd.DataFrame({\n",
    "    'name': range(n_ents),\n",
    "    'id': range(n_ents),\n",
    "})\n",
    "relation2id = pd.DataFrame({\n",
    "    'name': range(n_rels),\n",
    "    'id': range(n_rels),\n",
    "})\n",
    "time2id = pd.DataFrame({\n",
    "    'name': range(n_tws),\n",
    "    'id': range(n_tws),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-hop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_1_hop_pattern(\n",
    "    entity2id: pd.DataFrame,\n",
    "    relation2id: pd.DataFrame,\n",
    "    time2id: pd.DataFrame,\n",
    "    time_lags: 'List[Tuple(float,float)]',\n",
    "    seed: int = None,\n",
    ") -> TemporalPattern:\n",
    "    \"\"\" Create a 1-hop temporal pattern\n",
    "    (e1, r1, e2, t1) → (e3, r2, e4, t2)\n",
    "    Such that:\n",
    "        - e3 or e4 \\in {e1, e2}\n",
    "        - t2 > t1\n",
    "    Args:\n",
    "        entity2id (pd.DataFrame): Dataframe with entity ids in column 'id'\n",
    "        relation2id (pd.DataFrame): Dataframe with relation ids in column 'id'\n",
    "        time2id (pd.DataFrame): Dataframe with time window ids in column 'id'\n",
    "        time_lags (List[float]): Time lags with which antecedents and consequences\n",
    "            can occur validly. Either a list of float tuples or of functions that\n",
    "            can create such floats when called.\n",
    "        seed (int): Random seed, default None\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    # Randomly select all initial entities and relations to be used\n",
    "    sampled_entities = entity2id.sample(4, replace=True, random_state=seed)\\\n",
    "        ['id'].tolist()\n",
    "    sampled_relations = relation2id.sample(2, replace=True, random_state=seed)\\\n",
    "        ['id'].tolist()\n",
    "    # Define antecedent\n",
    "    antecedent = [\n",
    "        (\n",
    "            sampled_entities[0],  # e1\n",
    "            sampled_relations[0],  # r1\n",
    "            sampled_entities[1],  # e2\n",
    "        )\n",
    "    ]\n",
    "    # Consquence must satisfy the constraint that at least one of its entities is in\n",
    "    # some antecedent\n",
    "    if ~entities_intersect(sampled_entities[2:4], sampled_entities[:2]):\n",
    "        # Force at least of the consequent entities to switch to an antecedent entity\n",
    "        force_swap_to_entities([2,3], sampled_entities, sampled_entities[:2], seed)\n",
    "    consequence = (\n",
    "        sampled_entities[2],  # e3\n",
    "        sampled_relations[1],  # r2\n",
    "        sampled_entities[3],  # e4\n",
    "    )\n",
    "\n",
    "    time_lag_tuples = create_time_lag_tuples(time_lags, antecedent)\n",
    "\n",
    "    return TemporalPattern(\n",
    "        antecedent=antecedent,\n",
    "        consequence=consequence,\n",
    "        time_lags=time_lag_tuples,\n",
    "        n_hops=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(5, 4, 0, t1) -> (3, 0, 0, t2=t1+(1, 4))'"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pat = create_1_hop_pattern(\n",
    "    entity2id,\n",
    "    relation2id,\n",
    "    time2id,\n",
    "    [(0,4)],\n",
    "    # [(lambda: random.randint(1, 4), lambda: random.randint(5, 10))],\n",
    "    0\n",
    ")\n",
    "pat.__label__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-hop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_2_hop_pattern(\n",
    "    entity2id: pd.DataFrame,\n",
    "    relation2id: pd.DataFrame,\n",
    "    time2id: pd.DataFrame,\n",
    "    time_lags: 'List[Tuple(float,float)]',\n",
    "    seed: int = None,\n",
    ") -> TemporalPattern:\n",
    "    \"\"\" Create a 2-hop temporal pattern\n",
    "    (e1, r1, e2, t1) & (e3, r2, e4, t2) → (e5, r3, e6, t3)\n",
    "    Such that:\n",
    "        - (e3 or e4 \\in {e1, e2}) and (e5 or e6 \\in {e1, e2, e3, e4}) OR\n",
    "        - ~(e3 and e4 \\in {e1, e2}) and ((e5 \\in {e1, e2} and e6 \\in {e3, e4}) or\n",
    "            (e5 \\in {e3, e4} and e6 \\in {e1, e2}))\n",
    "        - t3 > t2 >= t1\n",
    "        - (e1, r1, e2, t1) != (e3, r2, e4, t2)\n",
    "\n",
    "    Args:\n",
    "        entity2id (pd.DataFrame): Dataframe with entity ids in column 'id'\n",
    "        relation2id (pd.DataFrame): Dataframe with relation ids in column 'id'\n",
    "        time2id (pd.DataFrame): Dataframe with time window ids in column 'id'\n",
    "        time_lags (List[float]): Time lags with which antecedents and consequences\n",
    "            can occur validly. Either a list of float tuples or of functions that\n",
    "            can create such floats when called.\n",
    "        seed (int): Random seed, default None\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    # Randomly select all initial entities and relations to be used\n",
    "    sampled_entities = entity2id.sample(6, replace=True, random_state=seed)\\\n",
    "        ['id'].tolist()\n",
    "    sampled_relations = relation2id.sample(3, replace=True, random_state=seed)\\\n",
    "        ['id'].tolist()\n",
    "    # Define antecedent\n",
    "    antecedent = [\n",
    "        (\n",
    "            sampled_entities[0],  # e1\n",
    "            sampled_relations[0],  # r1\n",
    "            sampled_entities[1],  # e2\n",
    "        ),\n",
    "        (\n",
    "            sampled_entities[2],  # e3\n",
    "            sampled_relations[1],  # r2\n",
    "            sampled_entities[3],  # e4\n",
    "        )\n",
    "    ]\n",
    "    # Second antecedent determines how we define the consequence\n",
    "    if entities_intersect(sampled_entities[2:4], sampled_entities[:2]):\n",
    "        # At least one entity intersects with the first antecedent\n",
    "        # Consquence must have at least one of its entities in some antecedent\n",
    "        if ~entities_intersect(sampled_entities[4:6], sampled_entities[:4]):\n",
    "            # Enforce that one or more consequence entity be chosen from the antecedents\n",
    "            force_swap_to_entities([4,5], sampled_entities, sampled_entities[:4], seed)\n",
    "    else:\n",
    "        # Non-intersecting first and second antecedents\n",
    "        # Consequence must connect them\n",
    "        if not entities_connect_triples(\n",
    "            sampled_entities[4], sampled_entities[5], antecedent[0], antecedent[1]\n",
    "        ):\n",
    "            # Enforce that the consequence connects the two antecedents\n",
    "            force_connect_components(\n",
    "                [4,5], sampled_entities, sampled_entities[:2], sampled_entities[2:4], seed\n",
    "            )\n",
    "    # Define consequence\n",
    "    consequence = (\n",
    "        sampled_entities[4],  # e5\n",
    "        sampled_relations[2],  # r3\n",
    "        sampled_entities[5],  # e6\n",
    "    )\n",
    "\n",
    "    time_lag_tuples = create_time_lag_tuples(time_lags, antecedent)\n",
    "    \n",
    "    return TemporalPattern(\n",
    "        antecedent=antecedent,\n",
    "        consequence=consequence,\n",
    "        time_lags=time_lag_tuples,\n",
    "        n_hops=2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(5, 4, 0, t1) & (3, 0, 3, t2=t1+(0, 4)) -> (0, 3, 3, t3=t2+(1, 5))'"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pat = create_2_hop_pattern(\n",
    "    entity2id,\n",
    "    relation2id,\n",
    "    time2id,\n",
    "    [(0,4), (0,5)],\n",
    "    # [(lambda: random.randint(1, 4), lambda: random.randint(5, 10))],\n",
    "    0\n",
    ")\n",
    "pat.__label__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-hop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_3_hop_pattern(\n",
    "    entity2id: pd.DataFrame,\n",
    "    relation2id: pd.DataFrame,\n",
    "    time2id: pd.DataFrame,\n",
    "    time_lags: 'List[Tuple(float,float)]',\n",
    "    seed: int = None,\n",
    ") -> TemporalPattern:\n",
    "    \"\"\" Create a 3-hop temporal pattern\n",
    "    (e1, r1, e2, t1) & (e3, r2, e4, t2) & (e5, r3, e6, t3) → (e7, r4, e8, t4)\n",
    "    Such that:\n",
    "        - All antecedents intersect: (e3 or e4 \\in {e1, e2}) and (e5 or e6 \\in {e1, e2, e3, e4})\n",
    "            and (e7 or e8 \\in {e1, e2, e3, e4, e5, e6})\n",
    "        - Second antecedent does not intersect first: ~(e3 and e4 \\in {e1, e2}) and\n",
    "            ((e5 \\in {e1, e2} and e6 \\in {e3, e4}) or (e5 \\in {e3, e4} and e6 \\in {e1, e2})) and\n",
    "            (e7 or e8 \\in {e1, e2, e3, e4, e5, e6})\n",
    "        - Third antecedent does not intersect first two: (e3 or e4 \\in {e1, e2}) and\n",
    "            ~(e5 and e6 \\in {e1, e2, e3, e4}) and ((e7 \\in {e1, e2, e3, e4} and e8 \\in {e5, e6})\n",
    "            or (e7 \\in {e5, e6} and e8 \\in {e1, e2, e3, e4}))\n",
    "        - t4 > t3 >= t2 >= t1\n",
    "        - (e1, r1, e2, t1) != (e3, r2, e4, t2) and (e1, r1, e2, t1) != (e5, r3, e6, t3) and\n",
    "            (e1, r1, e2, t1) != (e3, r2, e4, t2)\n",
    "\n",
    "    Args:\n",
    "        entity2id (pd.DataFrame): Dataframe with entity ids in column 'id'\n",
    "        relation2id (pd.DataFrame): Dataframe with relation ids in column 'id'\n",
    "        time2id (pd.DataFrame): Dataframe with time window ids in column 'id'\n",
    "        time_lags (List[float]): Time lags with which antecedents and consequences\n",
    "            can occur validly. Either a list of float tuples or of functions that\n",
    "            can create such floats when called.\n",
    "        seed (int): Random seed, default None\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    # Randomly select all initial entities and relations to be used\n",
    "    sampled_entities = entity2id.sample(8, replace=True, random_state=seed)\\\n",
    "        ['id'].tolist()\n",
    "    sampled_relations = relation2id.sample(4, replace=True, random_state=seed)\\\n",
    "        ['id'].tolist()\n",
    "    # Third antecedent must intersect at least one prior antecedent\n",
    "    if ~entities_intersect(sampled_entities[:4], sampled_entities[4:6]):\n",
    "        # Enforce that the third antecedent include entities from at least one prior\n",
    "        # antecedent\n",
    "        force_swap_to_entities([4,5], sampled_entities, sampled_entities[:4], seed)\n",
    "    # Define antecedent\n",
    "    antecedent = [\n",
    "        (\n",
    "            sampled_entities[0],  # e1\n",
    "            sampled_relations[0],  # r1\n",
    "            sampled_entities[1],  # e2\n",
    "        ),\n",
    "        (\n",
    "            sampled_entities[2],  # e3\n",
    "            sampled_relations[1],  # r2\n",
    "            sampled_entities[3],  # e4\n",
    "        ),\n",
    "        (\n",
    "            sampled_entities[4],  # e5\n",
    "            sampled_relations[2],  # r3\n",
    "            sampled_entities[5],  # e6\n",
    "        ),\n",
    "    ]\n",
    "    # Second and third antecedents determines how we define the consequence\n",
    "    if entities_intersect(sampled_entities[2:4], sampled_entities[:2]) & \\\n",
    "        entities_intersect(sampled_entities[4:6], sampled_entities[:4]):\n",
    "        # Antecedents are connected\n",
    "        # Consquence must have at least one of its entities in some antecedent\n",
    "        if (sampled_entities[6] not in sampled_entities[:6]) & \\\n",
    "            (sampled_entities[7] not in sampled_entities[:6]):\n",
    "            # Enforce that one or more consequence entity be chosen from the antecedents\n",
    "            force_swap_to_entities([6,7], sampled_entities, sampled_entities[:6], seed)\n",
    "    elif ~entities_intersect(sampled_entities[2:4], sampled_entities[:2]):\n",
    "        # Second antecedent does not intersect first antecedent\n",
    "        # Determine conditions for consequence based on how the third antecedent intersects\n",
    "        # the prior antecedents\n",
    "        if entities_connect_triples(\n",
    "            sampled_entities[4], sampled_entities[5], antecedent[0], antecedent[1]\n",
    "        ):\n",
    "            # Third antecedent connects prior antecedents\n",
    "            # Consquence must have at least one of its entities in some antecedent\n",
    "            if ~entities_intersect(sampled_entities[6:8], sampled_entities[:6]):\n",
    "                # Enforce that one or more consequence entity be chosen from the antecedents\n",
    "                force_swap_to_entities([6,7], sampled_entities, sampled_entities[:6], seed)\n",
    "        elif entities_intersect(sampled_entities[4:6], sampled_entities[:2]):\n",
    "            # Third antecedent connects with first antecedent\n",
    "            # Consequence must connect antecedents\n",
    "            if not entities_connect_components(\n",
    "                sampled_entities[6], sampled_entities[7],\n",
    "                [antecedent[0], antecedent[2]], [antecedent[1]]\n",
    "            ):\n",
    "                # Enforce that the consequence connects the disconnected antecedents\n",
    "                force_connect_components(\n",
    "                    [6,7], sampled_entities,\n",
    "                    sampled_entities[:2]+sampled_entities[4:6],\n",
    "                    sampled_entities[2:4],\n",
    "                    seed,\n",
    "                )\n",
    "        else:\n",
    "            # Third antecedent connects with second antecedent\n",
    "            # Consequence must connect antecedents\n",
    "            if not entities_connect_components(\n",
    "                sampled_entities[6], sampled_entities[7],\n",
    "                [antecedent[0]], [antecedent[1], antecedent[2]]\n",
    "            ):\n",
    "                # Enforce that the consequence connects the disconnected antecedents\n",
    "                force_connect_components(\n",
    "                    [6,7], sampled_entities,\n",
    "                    sampled_entities[:2],\n",
    "                    sampled_entities[2:6],\n",
    "                    seed,\n",
    "                )\n",
    "    else:\n",
    "        # Third antecedent does not intersect prior intersecting antecedents\n",
    "        # Consequence must connect them\n",
    "        if not entities_connect_components(\n",
    "            sampled_entities[6], sampled_entities[7],\n",
    "            [antecedent[0], antecedent[1]], [antecedent[2]]\n",
    "        ):\n",
    "            # Enforce that the consequence connects the disconnected antecedents\n",
    "            force_connect_components(\n",
    "                [6,7], sampled_entities, sampled_entities[:4], sampled_entities[4:6], seed,\n",
    "            )\n",
    "\n",
    "    # Define consequence\n",
    "    consequence = (\n",
    "        sampled_entities[6],  # e7\n",
    "        sampled_relations[3],  # r4\n",
    "        sampled_entities[7],  # e8\n",
    "    )\n",
    "    \n",
    "    time_lag_tuples = create_time_lag_tuples(time_lags, antecedent)\n",
    "\n",
    "    return TemporalPattern(\n",
    "        antecedent=antecedent,\n",
    "        consequence=consequence,\n",
    "        time_lags=time_lag_tuples,\n",
    "        n_hops=3,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(5, 4, 0, t1) & (3, 0, 3, t2=t1+(0, 4)) & (7, 3, 3, t3=t2+(0, 5)) -> (3, 3, 5, t4=t3+(1, 6))'"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pat = create_3_hop_pattern(\n",
    "    entity2id,\n",
    "    relation2id,\n",
    "    time2id,\n",
    "    [(0,4), (0,5), (0,6)],\n",
    "    # [(lambda: random.randint(0, 2), lambda: random.randint(0, 2)), (lambda: random.randint(0, 4), lambda: random.randint(0, 4)), (lambda: random.randint(0, 4), lambda: random.randint(0, 4))],\n",
    "    0\n",
    ")\n",
    "pat.__label__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose set of patterns\n",
    "n_3_hop = 10\n",
    "n_2_hop = 10\n",
    "n_1_hop = 10\n",
    "time_lag_3_hop = [\n",
    "    (0, lambda: scipy.stats.poisson(3).rvs(1)[0]),\n",
    "    (0, lambda: scipy.stats.poisson(3).rvs(1)[0]),\n",
    "    (1, lambda: scipy.stats.poisson(3).rvs(1)[0]),\n",
    "]\n",
    "time_lag_2_hop = [\n",
    "    (0, lambda: scipy.stats.poisson(3).rvs(1)[0]),\n",
    "    (1, lambda: scipy.stats.poisson(3).rvs(1)[0]),\n",
    "]\n",
    "time_lag_1_hop = [\n",
    "    (1, lambda: scipy.stats.poisson(3).rvs(1)[0]),\n",
    "]\n",
    "max_retries = 10\n",
    "# Start from 3-hop patterns, then 2-hop, then 1-hop\n",
    "# Prohibit any new patterns from being contained (antecedent and consequence) in the antecedent of\n",
    "# an existing larger pattern or being identical to an already chosen same-sized pattern\n",
    "patterns = []\n",
    "pattern_quadruples = []\n",
    "for _ in range(n_3_hop):\n",
    "    new_pat = False\n",
    "    retry = 0\n",
    "    while (not new_pat) | (retry > max_retries):\n",
    "        pat = create_3_hop_pattern(entity2id, relation2id, time2id, time_lag_3_hop)\n",
    "        # Find out if quadruple is \n",
    "        quad = pat.__quadruples__()\n",
    "        if ~is_subpattern(quad, pattern_quadruples):\n",
    "            patterns.append(pat)\n",
    "            pattern_quadruples.append(quad)\n",
    "            new_pat = True\n",
    "        retry += 1\n",
    "for _ in range(n_2_hop):\n",
    "    new_pat = False\n",
    "    retry = 0\n",
    "    while (not new_pat) | (retry > max_retries):\n",
    "        pat = create_2_hop_pattern(entity2id, relation2id, time2id, time_lag_2_hop)\n",
    "        # Find out if quadruple is \n",
    "        quad = pat.__quadruples__()\n",
    "        if ~is_subpattern(quad, pattern_quadruples):\n",
    "            patterns.append(pat)\n",
    "            pattern_quadruples.append(quad)\n",
    "            new_pat = True\n",
    "        retry += 1\n",
    "for _ in range(n_1_hop):\n",
    "    new_pat = False\n",
    "    retry = 0\n",
    "    while (not new_pat) | (retry > max_retries):\n",
    "        pat = create_1_hop_pattern(entity2id, relation2id, time2id, time_lag_1_hop)\n",
    "        # Find out if quadruple is \n",
    "        quad = pat.__quadruples__()\n",
    "        if ~is_subpattern(quad, pattern_quadruples):\n",
    "            patterns.append(pat)\n",
    "            pattern_quadruples.append(quad)\n",
    "            new_pat = True\n",
    "        retry += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pattern</th>\n",
       "      <th>n_hops</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(2, 3, 3, t1) &amp; (9, 2, 4, t2=t1+(0, 3)) &amp; (8, ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(8, 1, 6, t1) &amp; (2, 0, 4, t2=t1+(0, 9)) &amp; (4, ...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(2, 1, 2, t1) &amp; (9, 1, 5, t2=t1+(0, 6)) &amp; (2, ...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             pattern  n_hops  id\n",
       "0  (2, 3, 3, t1) & (9, 2, 4, t2=t1+(0, 3)) & (8, ...       3   0\n",
       "1  (8, 1, 6, t1) & (2, 0, 4, t2=t1+(0, 9)) & (4, ...       3   1\n",
       "2  (2, 1, 2, t1) & (9, 1, 5, t2=t1+(0, 6)) & (2, ...       3   2"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dataframe of pattern ids\n",
    "pattern2id = pd.DataFrame({\n",
    "    'pattern': [pat.__label__() for pat in patterns],\n",
    "    'n_hops': [pat.n_hops for pat in patterns],\n",
    "    'id': range(len(patterns)),\n",
    "})\n",
    "pattern2id[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "1. Start with random graph. Parameters: # of entities, # of relations, # of time windows, avg density of entities \n",
    "2. Choose set of patterns. Start w/ 3-hop, 2-hop, 1-hop. Pick time-stamp diffs. Don't allow subset patterns (e.g. chosen 2-hop patterns can't be included in 3-hop)\n",
    "3. Apply patterns iteratively. Parameters: Probability of random wiring, probability of no time pattern application (or interrupted time pattern application)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "454"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Density with which we randomly wire entities in this time window\n",
    "rnd_avg_density = 3\n",
    "# Function that returns an integer to be used for average density per entity\n",
    "rnd_avg_density_distr = None #lambda: scipy.stats.poisson.rvs(3, size=1)[0]\n",
    "# Probability that we do not apply a given pattern, per valid pattern (with all antecedents\n",
    "# satisfied in previous time windows)\n",
    "p_skip_consequence = .1\n",
    "# Probability that we create artificially create edges that validate a given pattern\n",
    "# (create edges that satisfay all antecedents), per pattern\n",
    "n_hops2p_force = {\n",
    "    1: .1,\n",
    "    2: .1,\n",
    "    3: .1,\n",
    "}\n",
    "# Note: A more sophisticated approach could also have a parameter to interrupt, or only\n",
    "# partially create edges that satisfy antecedents\n",
    "df_edgelist = pd.DataFrame({\n",
    "    'head': [],\n",
    "    'rel': [],\n",
    "    'tail': [],\n",
    "    't': [],\n",
    "    'wt': [],\n",
    "    'pattern': [],\n",
    "})\n",
    "for t in range(n_tws):\n",
    "    # First randomly wire entities\n",
    "    for ent_id in entity2id['id']:\n",
    "        # Sample entities to use as tails\n",
    "        if rnd_avg_density_distr:\n",
    "            dens = rnd_avg_density_distr()\n",
    "        else:\n",
    "            dens = int(rnd_avg_density)\n",
    "        tails = entity2id['id'].sample(dens, replace=True)\n",
    "        # Sample relations to connect them\n",
    "        rels = relation2id['id'].sample(dens, replace=True)\n",
    "        df_i = pd.DataFrame({\n",
    "            'head': [ent_id]*dens,\n",
    "            'rel': rels.values,\n",
    "            'tail': tails.values,\n",
    "            't': [t]*dens,\n",
    "            'wt': [1]*dens,\n",
    "            'pattern': [[]]*dens,\n",
    "        })\n",
    "        df_edgelist = pd.concat([\n",
    "            df_edgelist,\n",
    "            df_i,\n",
    "        ]).reset_index(drop=True)\n",
    "    \n",
    "    # Iterate over patterns\n",
    "    heads, rels, tails, pats = [], [], [], []\n",
    "    for label, pattern_id in zip(pattern2id['pattern'], pattern2id['id']):\n",
    "        # Instantiate pattern from label\n",
    "        pattern = TemporalPattern()\n",
    "        pattern.from_label(label)\n",
    "\n",
    "        # Artificially create valid patterns \n",
    "        rnd = random.random()\n",
    "        if rnd < n_hops2p_force[pattern.n_hops]:\n",
    "            # Create the antecedent in this and subsequent windows\n",
    "            # Track time window of current antecedent as we create them\n",
    "            t_i = int(t)\n",
    "            heads_pat, rels_pat, tails_pat, ts_pat = [], [], [], []\n",
    "            for antecedent, time_lag in zip(pattern.antecedent, pattern.time_lags):\n",
    "                heads_pat.append(antecedent[0])\n",
    "                rels_pat.append(antecedent[1])\n",
    "                tails_pat.append(antecedent[2])\n",
    "                ts_pat.append(t_i)\n",
    "                # Increment t_i according to time_lag min and max\n",
    "                t_i += random.randint(time_lag[0], time_lag[1])\n",
    "            df_pat = pd.DataFrame({\n",
    "                'head': heads_pat,\n",
    "                'rel': rels_pat,\n",
    "                'tail': tails_pat,\n",
    "                't': ts_pat,\n",
    "                'wt': [1]*len(heads_pat),\n",
    "                'pattern': [[pattern_id]]*len(heads_pat),\n",
    "            })\n",
    "            df_edgelist = pd.concat([\n",
    "                df_edgelist,\n",
    "                df_pat,\n",
    "            ]).reset_index(drop=True)\n",
    "        \n",
    "        # Apply valid patterns\n",
    "        rnd = random.random()\n",
    "        if rnd < p_skip_consequence:\n",
    "            # Skip the consequence even though antecedents may be satisfied\n",
    "            continue\n",
    "        # Test whether antecedents are satisfied in prior windows\n",
    "        antecedents_satisfied = False\n",
    "        # Iterate over antecedents in reverse order (most recent to least recent)\n",
    "        t_i = [t]  # Track current time window(s) for antecedent validation\n",
    "        for antecedent, time_lag in zip(pattern.antecedent[::-1], pattern.time_lags[::-1]):\n",
    "            # Check whether the antecedent exists in the edgelist\n",
    "            df_ants = [\n",
    "                df_edgelist[\n",
    "                    (df_edgelist['head'] == antecedent[0]) &\n",
    "                    (df_edgelist['rel'] == antecedent[1]) &\n",
    "                    (df_edgelist['tail'] == antecedent[2]) &\n",
    "                    (df_edgelist['t'] <= t_-time_lag[0]) & \n",
    "                    (df_edgelist['t'] >= t_-time_lag[1])\n",
    "                ] for t_ in t_i\n",
    "            ]\n",
    "            df_ants = [df for df in df_ants if df.shape[0] > 0]\n",
    "            if len(df_ants) == 0:\n",
    "                # No satisfied antecedent\n",
    "                antecedents_satisfied = False\n",
    "                continue\n",
    "            t_i = pd.concat(df_ants)['t'].unique().tolist()\n",
    "            antecedents_satisfied = True\n",
    "        # If all antecedents are satisfied, create consequence in current time window\n",
    "        if antecedents_satisfied:\n",
    "            heads.append(pattern.consequence[0])\n",
    "            rels.append(pattern.consequence[1])\n",
    "            tails.append(pattern.consequence[2])\n",
    "            pats.append([pattern_id])\n",
    "            # Note: Could try to label antecedent edges which satisfied this pattern with\n",
    "            # relevant pattern id. But I found this really difficult to track, since the\n",
    "            # way I track antecedent validity is like a branching tree.\n",
    "    # Add all new consequences to edgelist\n",
    "    df_pat = pd.DataFrame({\n",
    "        'head': heads,\n",
    "        'rel': rels,\n",
    "        'tail': tails,\n",
    "        't': [t]*len(heads),\n",
    "        'wt': [1]*len(heads),\n",
    "        'pattern': pats,\n",
    "    })\n",
    "    df_edgelist = pd.concat([\n",
    "        df_edgelist,\n",
    "        df_pat,\n",
    "    ]).reset_index(drop=True)\n",
    "\n",
    "# Cut off df_edgelist at n_tws\n",
    "df_edgelist = df_edgelist[df_edgelist['t'] < n_tws]\n",
    "# Aggregate duplicate edges\n",
    "df_edgelist = df_edgelist.groupby(['head', 'rel', 'tail', 't']).agg({\n",
    "    'wt': 'sum',\n",
    "    'pattern': lambda x: sorted(list(set([el for ids in x for el in ids]))),\n",
    "}).reset_index().sort_values(['t', 'head', 'tail', 'rel']).reset_index(drop=True)\n",
    "df_edgelist['head'] = df_edgelist['head'].astype(int)\n",
    "df_edgelist['rel'] = df_edgelist['rel'].astype(int)\n",
    "df_edgelist['tail'] = df_edgelist['tail'].astype(int)\n",
    "df_edgelist['t'] = df_edgelist['t'].astype(int)\n",
    "len(df_edgelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export all relevant files\n",
    "wd = '/nas/ckgfs/users/eboxer/tkg_patterns'\n",
    "export_dir = os.path.join(wd, 'scratch_output')\n",
    "entity2id.to_csv(\n",
    "    os.path.join(export_dir, 'entity2id.txt'), sep='\\t', index=False, header=False)\n",
    "relation2id.to_csv(\n",
    "    os.path.join(export_dir, 'relation2id.txt'), sep='\\t', index=False, header=False)\n",
    "with open(os.path.join(export_dir, 'stat.txt'), 'w') as f:\n",
    "    f.writelines(f'{entity2id.id.nunique()}\\t{relation2id.id.nunique()}\\t0')\n",
    "time2id.to_csv(\n",
    "    os.path.join(export_dir, 'timestamp2id.txt'), sep='\\t', index=False, header=False)\n",
    "pattern2id.to_csv(\n",
    "    os.path.join(export_dir, 'pattern2id.txt'), sep='\\t', index=False, header=False)\n",
    "df_edgelist.to_csv(\n",
    "    os.path.join(export_dir, 'edgelist.txt'), sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
